import abc
import bisect
import math
import random
from typing import (
    List,
    Optional,
    Tuple,
    Union,
    )

# matplotlib.use('tkagg')
import numpy as np

# sys.path.append('../examples/flatirons')
# import func_tools
from optimization.optimizer.ask_tell_optimizer import AskTellOptimizer


# import shapely


class DCEM(AskTellOptimizer):
    """
    A prototype implementation of the decomposed cross-entropy method.
    """
    
    class Dimension:
        
        @abc.abstractmethod
        def update(self, samples: [any]) -> None:
            pass
        
        @abc.abstractmethod
        def sample(self) -> Union[float, int]:
            pass
        
        @abc.abstractmethod
        def best(self) -> Union[float, int]:
            pass
    
    class GaussianDimension(Dimension):
        
        def __init__(self, mu: float, sigma: float, scale: float = 1.0):
            self.mu: float = mu
            self.sigma: float = sigma
            self.scale: float = scale
        
        def update(self, samples: [float]) -> None:
            self.mu = np.mean(samples, 0)
            self.sigma = np.std(samples, 0, ddof=1) * self.scale
        
        def sample(self) -> float:
            return random.gauss(self.mu, self.sigma)
        
        def best(self) -> Union[float, int]:
            return self.mu
    
    class MultinomialDimension(Dimension):
        
        def __init__(self, probabilities: [float]):
            self.cumulative_distribution: [float] = self._get_cumulative_distribution(probabilities)
        
        def update(self, samples: [int]) -> None:
            counts = [0] * len(self.cumulative_distribution)
            
            for sample in samples:
                counts[sample] = counts[sample] + 1
            
            cumulative_counts = self._get_cumulative_distribution(counts)
            
            total = float(len(samples))
            for i in range(len(cumulative_counts)):
                self.cumulative_distribution[i] = cumulative_counts[i] / total
        
        def sample(self) -> int:
            return bisect.bisect_right(self.cumulative_distribution, random.random())
        
        def best(self) -> Union[float, int]:
            # could be memoized at update time
            last = 0.0
            largest = (0.0, 0)
            for i, c in self.cumulative_distribution:
                p = c - last
                last = c
                if p >= largest[0]:
                    largest = (p, i)
            return largest[1]
        
        def _get_cumulative_distribution(self, probabilities):
            cumulative = [0] * len(probabilities)
            acc = 0
            for i, p in enumerate(probabilities):
                acc = acc + p
                self.cumulative_distribution[i] = acc
            return cumulative
    
    def __init__(self,
                 generation_size: int = 100,
                 selection_proportion: float = .33,
                 dimensions: Optional[List['DCEM.Dimension']] = None,
                 ) -> None:
        self.dimensions: [DCEM.Dimension] = [] if dimensions is None else dimensions
        self.generation_size: int = generation_size
        self.selection_proportion: float = selection_proportion
        self.best_candidate = None  # self.mu()
    
    def setup(self, dimension: [Dimension]) -> None:
        self.dimensions = dimension
    
    def stop(self) -> bool:
        """
        :return: True when the optimizer thinks it has reached a stopping point
        """
        return False
    
    def ask(self, num: Optional[int] = None) -> [any]:
        """
        :param num: the number of search points to return. If undefined, the optimizer will choose how many to return.
        :return: a list of search points generated by the optimizer
        """
        num = self.generation_size if num is None else num
        
        population = []
        for _ in range(num):
            candidate = [0.0] * len(self.dimensions)
            for i, dimension in enumerate(self.dimensions):
                candidate[i] = dimension.sample()
            population.append(candidate)
        
        return population
    
    def tell(self, evaluations: [Tuple[float, any]]) -> None:
        """
        Updates the optimizer with the objective evaluations of a list of search points
        :param evaluations: a list of tuples of (evaluation, search point)
        """
        evaluations.sort(key=lambda evaluation: evaluation[0], reverse=True)
        if self.best_candidate is None or evaluations[0][0] > self.best_candidate[0]:
            self.best_candidate = evaluations[0]
        
        selection_size = math.ceil(self.selection_proportion * len(evaluations))
        del evaluations[selection_size:]
        
        for i, dimension in enumerate(self.dimensions):
            dimension.update([evaluation[1][i] for evaluation in evaluations])
    
    def best(self) -> any:
        """
        :return: the current best solution
        """
        return self.mu() if self.best_candidate is None else self.best_candidate[1]
    
    def mu(self) -> any:
        """
        :return: the current best solution
        """
        return [dimension.best() for dimension in self.dimensions]

    def get_num_candidates(self) -> Optional[int]:
        return self.generation_size
